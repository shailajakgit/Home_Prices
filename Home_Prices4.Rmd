---
title: "Home_Prices4"
author: "Shailaja_K"
date: "June 16, 2017"
output:
  pdf_document: default
  word_document: default
---


```{r setup, cache = F}
knitr::opts_chunk$set(error = TRUE)
suppressWarnings(library(dplyr))
suppressWarnings(library(ggplot2))
suppressWarnings(library(tidyr))
suppressWarnings(library(rpart))
suppressWarnings(library(rpart.plot))
suppressWarnings(library(poLCA))
suppressWarnings(library(AER))
suppressWarnings(library(randomForest))
suppressWarnings(library(caret))
suppressWarnings(library(ALS))
suppressWarnings(library(Matrix))
suppressWarnings(library(relaimpo))

```


--------------------------------------
Distribution of the Dependent Variable 
--------------------------------------

The dependent variable is SalePrice. Since parametric models give very good interpretation, this section contains multiple checks to see if the dependent variable fits any known theoritical distribution. log-log transformation of the response variable makes it aproximately gaussian.

First, Plot the histogram of SalePrice to identify the distribution.

```{r read data}
trainHouse <- read.csv("./Data/train.csv", header = T)
# testHouse <- read.csv("C:/GitHub_Local/Home_Prices/test.csv", header = T)

# Commenting out. This is line producing a long output.
# glimpse(trainHouse)
# summary(trainHouse)
dim(trainHouse)

# Remove Id variable 
trainHouse <- trainHouse[,!colnames(trainHouse) %in% c("Id")]
dim(trainHouse)
hist(trainHouse$SalePrice)
```



The house prices are skewed to the right side. Let us try fitting a set of skewed distributions to SalePrice and 
determine if the fit is appropriate using kolmogorov-smirnov test.

1. Log-normal: Price is a real valued variable. Log-normal is a skewed distribution, typically applied to prices. The following function performs a 1000 ks.tests for the given data vector with the given distribution. Since the test depends on random number generation, the ks.tests are performed multiple times, instead of performing just once.
```{r Fit distributions, warning=FALSE, message=FALSE}
set.seed(0)

# 1. Log-normal
# A function to run 1000 ks.tests.
fitDist1000 <- function(vec,fun,params){
  counter = 0
  size = length(vec)
  listofParams <- lapply(c(size,params), function(x){x})
  for(i in c(1:1000)){
    
  res <- ks.test(vec, do.call(match.fun(fun),listofParams))
  
  if (res$p.value > 0.05){
    counter = counter+1
    }
  }
  return(counter)
}

fit.lognorm.Params <- fitdistr(trainHouse$SalePrice, "lognormal")
(fitDist1000(trainHouse$SalePrice,"rlnorm",fit.lognorm.Params$estimate))


```

Null hypothesis of the ks.test is that the two input vectors have the same distribution. But at 95% confidence level, the
null hypothesis is not rejected 63% times. Let us make sure visually that SalePrice distribution looks like log-normal:

```{r Fit distributions-2}
logSalePrice <- log(trainHouse$SalePrice)
qqnorm(logSalePrice)
qqline(logSalePrice)

plot(density(logSalePrice))
```

QQ-plot shows that log(SalesPrice) has fatter tails compared to the normal distribution. The distribution is still skewed to the right even after taking log. 


2. Loglog transformation: Applying log twice:
```{r log-log, warning=FALSE, message=FALSE}
# loglog <- log(log(trainHouse$SalePrice))
# hist(loglog)
logSalePrice <- log(trainHouse$SalePrice)

set.seed(0)
fit.loglognorm.Params <- fitdistr(logSalePrice, "lognormal")
(fitDist1000(logSalePrice,"rlnorm",fit.loglognorm.Params$estimate))

qqnorm(log(logSalePrice))
qqline(log(logSalePrice))
plot(density(log(logSalePrice)))

```

Though the density plot and qqplot look similar to those of logSalePrice, loglog transformation seem to fit lognormal distribution better. ks.test could not reject the null hypothesis 74.5% of the time.


```{r, log and log-log}

hist(log(trainHouse$SalePrice))
hist(log(log(trainHouse$SalePrice)))

```

Both log and loglog transformed values have almost symmetric distribution with fat tails. So trying t-distribution on log and loglog transformed data might bring the distribution closer to gaussian distribution.

4. t- distribution
```{r t- distribution, warning=FALSE, message=FALSE}
set.seed(0)
logSalePrice <- log(trainHouse$SalePrice)
fit.pois.Params <- fitdistr(logSalePrice, "t")
(fitDist1000(logSalePrice,"rt",(fit.pois.Params$estimate)[3]))


set.seed(0)
loglogSalePrice <- log(log(trainHouse$SalePrice))
fit.pois.Params <- fitdistr(loglogSalePrice, "t")
(fitDist1000(loglogSalePrice,"rt",(fit.pois.Params$estimate)[3]))

```
The log and log-log transformations do not fit t-distribution.

3. Poisson: Even though Poisson distribution is not appropriate for real valued variables, I would like to try fitting Poisson distribution to sales price:

```{r poisson distribution, warning=FALSE, message=FALSE}
set.seed(0)
fit.pois.Params <- fitdistr(trainHouse$SalePrice, "Poisson")
(fitDist1000(trainHouse$SalePrice,"rpois",fit.pois.Params$estimate))

```
4. Negative binomial distribution

```{r negative binomial distribution, warning=FALSE, message=FALSE, error=FALSE,cache = F}
# set.seed(0)
# fit.pois.Params <- fitdistr(trainHouse$SalePrice, "negative binomial")
# (fitDist1000(trainHouse$SalePrice,"rnbinom",fit.pois.Params$estimate))
# 
# scaledSalePrice <- scale(trainHouse$SalePrice)
# hist(scaledSalePrice)

```

SalePrice could not fit negative binomial distribution. The system became singular. Scaling the variable resulted in negative values, but negative binomial expects positive values, so can't fit negative binomial to scaled values.


```{r gamma distribution, warning=FALSE, message=FALSE}
set.seed(0)
fit.pois.Params <- fitdistr(trainHouse$SalePrice, "gamma",list(shape = 1, rate = 0.1), lower = 0.01)
(fitDist1000(trainHouse$SalePrice,"rgamma",fit.pois.Params$estimate))

```

Gamma distribution did not fit, either.

---------------------------------------------
2. Cleaning the missing data as appropriate.
---------------------------------------------

```{r check for missingness}
# Data prep
logtrainHouse <- trainHouse
# Add logSalePrice column to the dataset
logtrainHouse$logSalePrice <- log(trainHouse$SalePrice)

# Remove SalePrice column
logtrainHouse <- logtrainHouse[, !colnames(logtrainHouse) %in% c("SalePrice")]

# Make the logSalePrice column to be the first column.
logtrainHouse <- logtrainHouse[,c(80,c(1:79))]
#colnames(logtrainHouse)

# Count the missing values in all columns
missingValuesinColumns <- apply(logtrainHouse,2, function(x){sum(is.na(x))})
missingValuesinColumns[missingValuesinColumns>0]
dim(logtrainHouse)

sum(missingValuesinColumns[missingValuesinColumns>0])
# sum(missingValuesinColumns>0)
```
Alley, PoolQC, Fence, MiscFeature - These variables have more than 80% values missing. Imputing them from the available
values would be unrealistic. Therefore, deleting these columns.

```{r Remove missing columns}
logtrainHouse <- logtrainHouse[,c(c(1:6),c(8:57),c(59:72),c(76:80))]
#colnames(logtrainHouse)[4]
```
Garage related fields do not seem to be missing at random. All garage related fields have (almost) equal number of values missing. Let us investigate further:

```{r, garage missing}
# colnames(logtrainHouse)

# Give 1 to each cell of the df with a missing value
missingDF <- as.data.frame(abs(is.na(logtrainHouse)))

# Extract columns with missing values. sapply applies mean function to each column and returns 
# 0 or a positive value indicating no nulls and nulls, respectively.
onlyMissingDF <- missingDF[sapply(missingDF, mean) > 0 ] %>% dplyr::select(contains('Garage'))
head(onlyMissingDF,2)

# Check the relationship of these variables:
cor(onlyMissingDF)

head(logtrainHouse[logtrainHouse$GarageArea==0,]%>% dplyr::select(contains('Garage')),2)

# a<- c(10,10,10)
# cor(a)

```

The correlation value 1 shows that Garage related fields are not missing at random at all! The Garage related attributes, such as finish and yearbuilt are missing because there is no garage in these houses.

But instead of removing 81 rows with missing garage related attributes,
only the variable indicating garage presence, which has no null values - GarageArea - can be included.
```{r Garage}
head(logtrainHouse[is.na(logtrainHouse$GarageType),] %>% dplyr::select(contains("Garage")),2)

# Exclude all garage related fileds except GarageArea
logtrainHouse <- logtrainHouse %>% dplyr::select(-starts_with('Garage'), GarageArea)

# colnames(logtrainHouse)
```

Imputation: Convert year to age. Put 999 where there is no garage. Cars = 0, remove the finish column.

Check the missingess of Basement related fields:
```{r Basement}
head(logtrainHouse[is.na(logtrainHouse$BsmtQual),] %>% dplyr::select(contains("Bsmt")),2)

logtrainHouse <- logtrainHouse %>% dplyr::select(-starts_with('Bsmt'),BsmtFinSF1,BsmtFinSF2)

# colnames(logtrainHouse)
```


Basement related fields are also not missing at random. Excluding all additional parameteres related to basement except BasementFinSF.


LotForntage has 259 values missing. Investigate the nature of missingness. 

```{r missing lotfrontage}
# colnames(onlyMissingDF)

# Give 1 to each cell of the df with a missing value
missingDF <- as.data.frame(abs(is.na(logtrainHouse)))

# Extract columns with missing values. sapply applies mean function to each column 
# and returns 0 or a positive value indicating no nulls and nulls, respectively.
onlyMissingDF <- missingDF[sapply(missingDF, mean) > 0 ]
head(onlyMissingDF,2)

# Missingmess in LotFrontage is not coinciding with missingness in any other column. 
# This may be missing at random. Remove 259 rows from the dataset.
cor(onlyMissingDF)

# dim(logtrainHouse)
logtrainHouse <- logtrainHouse[!is.na(logtrainHouse$LotFrontage),]

```

Missingness of LotFrontage does not seem to coincide with others. This could be missing at random. Therefore, removing the rows with missing values in LotFrontage column.

Eventhough missingness in MasVnrArea and MasVnrType are coinciding, the number of rows with missing values are  small. Removing those rows may not impact the solution much.

Check the missingness of the rest of the data frame. Since the number of rows with missing data is small, remove the rows. 

```{r other missing columns-3}
missingValuesinColumns <- apply(logtrainHouse,2, function(x){sum(is.na(x))})
missingValuesinColumns

logtrainHouse <- logtrainHouse[apply(logtrainHouse,1, function(x){sum(is.na(x))==0}),]
# dim(logtrainHouse)

# sum(is.na(logtrainHouse))

```



All missing values have been eliminated. 

Some of the variables are 'Year' vaules, converting them to duration would be appropriate.

```{r Year to age}

# Replace year column with age, w.r.t 2017.

yearToAge <- (logtrainHouse %>% dplyr::select(contains('Year'),YrSold) %>% 
                mutate(BuiltBefore = (2017 - YearBuilt), 
                       RemodBefore = (2017 - YearBuilt), 
                       SoldBefore = (2017 - YrSold)) %>% 
                dplyr::select(BuiltBefore, RemodBefore,SoldBefore))

logtrainHouse <- logtrainHouse %>% dplyr::select(-contains('Year'))
logtrainHouse <- logtrainHouse %>% dplyr::select(-YrSold)

logtrainHouse$BuiltBefore <- yearToAge$BuiltBefore
logtrainHouse$RemodBefore <-  yearToAge$RemodBefore
logtrainHouse$SoldBefore <- yearToAge$SoldBefore
logtrainHouse <- logtrainHouse[,!colnames(logtrainHouse) %in% c("MoSold")]

```

The above code also removes 'Month sold' column, since 'Year Sold' variable is already present in the dataset, this variable do not add much value.

Let us check the distribution of the dependent variable again.


```{r check the distribution again}

hist(logtrainHouse$logSalePrice)
hist(log(logtrainHouse$logSalePrice))

```

Check if the cleansed data fits t-distribution.
```{r t-distribution cleansed data, warning=FALSE, message=FALSE}
set.seed(0)
logSalePrice <- logtrainHouse$logSalePrice
fit.pois.Params <- fitdistr(logSalePrice, "t")
(fitDist1000(logSalePrice,"rt",(fit.pois.Params$estimate)[3]))


set.seed(0)
loglogSalePrice <- log(logSalePrice)
fit.pois.Params <- fitdistr(loglogSalePrice, "t")
(fitDist1000(loglogSalePrice,"rt",(fit.pois.Params$estimate)[3]))

# colnames(logtrainHouse)

```
t-distribution does not fit the data.

Check again what transformation fits the response variable better with lognormal distribution.
```{r log or loglog, warning=FALSE, message=FALSE}
set.seed(0)
logSalePrice <- logtrainHouse$logSalePrice
fit.pois.Params <- fitdistr(logSalePrice, "normal")
(fitDist1000(logSalePrice,"rnorm",fit.pois.Params$estimate))


set.seed(0)
loglogSalePrice <- log(logSalePrice)
fit.pois.Params <- fitdistr(loglogSalePrice, "normal")
(fitDist1000(loglogSalePrice,"rnorm",fit.pois.Params$estimate))


```
Log-log transformation seems to work well even with the truncated data.

--------------------------------
3. Model fitting
--------------------------------

1. GLM (with gaussian family)

```{r, glm, warning=TRUE, error=TRUE}
model2 <- glm(log(logSalePrice)~. , data = logtrainHouse)

summary(model2)

```

But glm is failing because 'Utilities' column is categorical and has a single level. Since all the values are the same, it doesn't explain any variance in logSalePrice column. Therefore, removing the column.

```{r, remove attribute with a single level}
factorsWith1Level <- function(x){
  if(is.factor(x)){
    return(length(unique(x)) == 1)
  }
  else{
    return(FALSE)
  }
}

names(which(sapply(logtrainHouse, factorsWith1Level)))

logtrainHouse <- logtrainHouse[,-which(sapply(logtrainHouse, factorsWith1Level))]
```


glm with log and loglog SalePrice as response variable:


```{r compare log and loglog models}
# Test if glm fails again by trying log model
loglinearModel <- glm(logSalePrice~.,data = logtrainHouse)

# log-log model
logloglinearModel <- glm(log(logSalePrice)~.,data = logtrainHouse)


(c(logModelAIC=loglinearModel$aic, loglogModelAIC=logloglinearModel$aic))

```
loglog transformation of the dependent variable results in lower AIC for the given dataset. Therefore, I am pursuing loglog model further. 

```{r log-log model,warning=TRUE,  error=TRUE}
# ?train
# summary(logloglinearModel)
trCtrl <- trainControl(method = "cv", number = 10)
# summary(lm(log(logSalePrice)~.,data = logtrainHouse))
lm.cv <- train(log(logSalePrice)~.,
               data = logtrainHouse,
               method = "lm",
               trControl = trCtrl)

lm.cv$results
summary(lm.cv$finalModel)


```



LM resulted in many insignificant parameters, though it has 76% R-squared. Some levels of categorical factors are insignificant. We can work around this by combining the levels step by step. That requires many iterations give the large number of factors in some categorical variables.

Instead, we can try a non-parametric regression that deals with this issue and handles interactions between variables as well.

2. Random Forest
Since the trees in random forest minimize the sum of squares, it is better to use the transformed variable whose distribution is closer to normal distribution.

```{r, random forest,eval=FALSE}
# dim(logtrainHouse)
trCtrl <- trainControl(method = "cv",number=5)

# Try multiple mtry
rf1Grid <-  expand.grid(mtry = c(25,30,50))

# append the cv results to the below data frame
modelResults <- data.frame(mtry=0,RMSE=0,Rsquared=0,RMSESD=0,RsquaredSD=0)

# Random forest iterations with different number of trees and mtry values
for (i in seq(100,300,by = 50)){
  rf1 <- train(log(logSalePrice)~.,
      data = logtrainHouse,
      method = "rf", 
      trControl = trCtrl,
      tuneGrid = rf1Grid,
      ntrees = i,
      verbose = F)
  modelResults <- rbind(modelResults,rf1$results)
}

# First row contains all zeros. Removing it.
modelResults <- modelResults[-1,]

# Find the combination of variables with lowest RMSE.
modelResults[which.min(modelResults$RMSE),]

modelResults

```


Though the best test RMSE has occured with mtry = 50 (all the variables) and number of trees = 300, a smaller model with mtry =50 and 100 trees is almost as good as the best model. The smaller model has slightly high RMSE, but it achieves this results with fewer trees.

In comparision with linear model, random forest performed better. RMSE of lm = 0.01798476,  whereas RMSE of random forest = 0.01217267.

Fit the final random forest with the best parameters.

```{r, random forest-final}
set.seed(0)
randForest <- randomForest(log(logSalePrice)~.,data = logtrainHouse, mtry = 50, ntree= 100)

# randForest$mse
# randForest$rsq
# randForest$forest


(ForestRMSE = (sum((randForest$y - randForest$predicted)^2)/nrow(logtrainHouse))^0.5)

varImpPlot(randForest)


```

The following partial dependece plots show the marginal impact of a predictor on the dependent variable.
```{r, partial dependence plots}

# variableImportance <- randForest$importance[order(randForest$importance,decreasing = T),]
# sapply(names(variableImportance[1:5]), function(x) {partialPlot(randForest, logtrainHouse,x)})
# sapply(variableImportance[1:5], function(x) {partialPlot(randForest, logtrainHouse,x)})


partialPlot(randForest, logtrainHouse,OverallQual)
partialPlot(randForest, logtrainHouse,GrLivArea)

partialPlot(randForest, logtrainHouse,TotalBsmtSF)
partialPlot(randForest, logtrainHouse,Neighborhood)


```

These partial dependence plots show some interesting insights. 
1. As the overall quality of the increases, price goes high. The effect is more dramatic for the higher quality houses (>6).
2. Prices increase as the living area of the house inceases.
3. Total basement square footage matters only upto a point. After 2000 sqft, it doesn't seem to affect the price much.
4. Neighborhood shows a counter intuitive effect, though. According to the plot, neighborhood does impact the price. But in reality, house prices heavily depend on locality.

























